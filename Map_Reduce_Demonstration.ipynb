{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map reduce is very functional algorithm where three parts of it can easily executed on different machines. In this assignment, we will try to implement this algorithm into a >100 million rows dataset.\n",
    "\n",
    "For comparison, we will have two ways of getting counts of carriers.\n",
    "\n",
    "1. Serial way - Looping through each record and counting each airline's flight\n",
    "2. Map reduce way - map, reduce and sort, and collect way to counn the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import utils\n",
    "import data_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data downloaded. you can skip this step or delete data folder to download again.\n"
     ]
    }
   ],
   "source": [
    "data_handler.download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all files under data folder\n",
    "file_list = sorted(glob.glob(os.path.join('data', '*.csv.bz2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Way\n",
    "\n",
    "Here, we are getting a list of files under the data folder. The serial way requires to loop through all of the records and update one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (7009728, 29) ; 1626.26 Mb [####################] 100% \n"
     ]
    }
   ],
   "source": [
    "for ind, data_file in enumerate(file_list):\n",
    "    # read current data\n",
    "    df = pd.read_csv(data_file, encoding='ISO-8859-1', memory_map=True, low_memory=False)\n",
    "    \n",
    "    # unique airlines in dataset\n",
    "    carriers = df.UniqueCarrier.unique()\n",
    "    \n",
    "    # update the global carrier_count\n",
    "    for key in carriers:\n",
    "        if key not in carrier_counts:\n",
    "            carrier_counts.update({key: 0})\n",
    "    \n",
    "    # loop through each row in dataframe \n",
    "    for carrier in df.UniqueCarrier:\n",
    "        carrier_counts[carrier] += 1\n",
    "\n",
    "    # info \n",
    "    prefix ='Shape: {} ; {} Mb'.format(\n",
    "        df.shape, round(df.memory_usage().sum() / 1e+6,2))\n",
    "    \n",
    "    # track the progress\n",
    "    utils.progressbar(len(file_list), ind + 1, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PS': 83617,\n",
       " 'TW': 3757747,\n",
       " 'UA': 13299817,\n",
       " 'WN': 15976022,\n",
       " 'EA': 919785,\n",
       " 'HP': 3636682,\n",
       " 'NW': 10292627,\n",
       " 'PA (1)': 316167,\n",
       " 'PI': 873957,\n",
       " 'CO': 8145788,\n",
       " 'DL': 16547870,\n",
       " 'AA': 14984647,\n",
       " 'US': 14075530,\n",
       " 'AS': 2878021,\n",
       " 'ML (1)': 70622,\n",
       " 'AQ': 154381,\n",
       " 'MQ': 3954895,\n",
       " 'OO': 3090853,\n",
       " 'XE': 2350309,\n",
       " 'TZ': 208420,\n",
       " 'EV': 1697172,\n",
       " 'FL': 1265138,\n",
       " 'B6': 811341,\n",
       " 'DH': 693047,\n",
       " 'HA': 274265,\n",
       " 'OH': 1464176,\n",
       " 'F9': 336958,\n",
       " 'YV': 854056,\n",
       " '9E': 521059}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results\n",
    "carrier_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Way\n",
    "\n",
    "So This dataset is very big have millions of records that's why i will use two columns 'FlightNum', 'UniqueCarrier' for creating a dataframe for running the MapReduce Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import groupby\n",
    "\n",
    "data = pd.DataFrame()\n",
    "cols_to_Load = ['FlightNum', 'UniqueCarrier']\n",
    "for ind, data_file in enumerate(file_list):\n",
    "    df = pd.DataFrame(pd.read_csv(data_file, encoding='ISO-8859-1', memory_map=True, low_memory=False, usecols= cols_to_Load))\n",
    "    data=data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers = data.UniqueCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 1.  Mapping\n",
    "\n",
    "mapping=map(lambda x:(x,1),carriers) \n",
    "#print(list(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2.  Shuffling or Sorting\n",
    "\n",
    "sorted_mapping=sorted(mapping)\n",
    "print(sorted_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9E', 521059), ('AA', 14984647), ('AQ', 154381), ('AS', 2878021), ('B6', 811341), ('CO', 8145788), ('DH', 693047), ('DL', 16547870), ('EA', 919785), ('EV', 1697172), ('F9', 336958), ('FL', 1265138), ('HA', 274265), ('HP', 3636682), ('ML (1)', 70622), ('MQ', 3954895), ('NW', 10292627), ('OH', 1464176), ('OO', 3090853), ('PA (1)', 316167), ('PI', 873957), ('PS', 83617), ('TW', 3757747), ('TZ', 208420), ('UA', 13299817), ('US', 14075530), ('WN', 15976022), ('XE', 2350309), ('YV', 854056)]\n"
     ]
    }
   ],
   "source": [
    "# Phase 3. Reducing\n",
    "grouper=groupby(sorted_mapping, lambda p:p[0])\n",
    "final=map(lambda l:(l[0],reduce(lambda x, y:x+y,map(lambda p: p[1], l[1]))), grouper)\n",
    "\n",
    "print(list(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we did two ways of getting counts of carriers.\n",
    "\n",
    "1.Serial way - Looping through each record and counting each airline's flight\n",
    "\n",
    "2.Map reduce way - map, reduce and sort, and collect way to count the flights\n",
    "Mapping involves processing a large data set parallelly to generate <key,value> pairs. These <key,value> pairs are fed to \n",
    "reduce which combines the data tuples into a smaller set.Word Count is one of the simplest applications of MapReduce. \n",
    "Here we have a huge dataset and we want to count the frequency of words. We run Map on this dataset to generate <key,value> \n",
    "pairs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
